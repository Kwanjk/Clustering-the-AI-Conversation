{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f57afb-09e9-4993-a8a0-7047ef37c2b8",
   "metadata": {},
   "source": [
    "### Clustering Public Perception of Artificial Intelligence\n",
    "### Author: Joshua Kwan\n",
    "### Goal: Collect, clean, and cluster public discussions about AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bd72d3-9ddb-4760-b63e-04dc3a1e1c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Thanos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thanos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. LIBRARIES\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c19a82a-b652-4ed9-a0f9-38cc60b07f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Prep Folders \n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51871a98-0451-4220-b0dd-c724cf52b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD OR COLLECT DATA \n",
    "\n",
    "# Option A: Load pre-downloaded Kaggle / JSON data\n",
    "# df = pd.read_csv(\"data/ai_discussions.csv\")\n",
    "\n",
    "# Option B: Placeholder dataframe if scraping manually\n",
    "data = {\n",
    "    \"post_id\": [1, 2, 3],\n",
    "    \"text\": [\n",
    "        \"AI is changing everything, this is revolutionary!\",\n",
    "        \"ChatGPT scares me... it's like Ultron becoming real.\",\n",
    "        \"AI is useful but not magic, people exaggerate its power.\"\n",
    "    ],\n",
    "    \"upvotes\": [120, 45, 66]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63640537-0421-479c-a485-af8bde541538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Data \n",
    "\n",
    "# 3. Collect Reddit posts (public JSON API, no login required)\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_posts(subreddit, limit=200):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}\"\n",
    "    headers = {\"User-Agent\": \"ai-clustering-demo/0.1\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"⚠️  Error fetching {subreddit}: {res.status_code}\")\n",
    "        return []\n",
    "    data = res.json()[\"data\"][\"children\"]\n",
    "    return [\n",
    "        {\n",
    "            \"subreddit\": subreddit,\n",
    "            \"title\": p[\"data\"][\"title\"],\n",
    "            \"selftext\": p[\"data\"].get(\"selftext\", \"\"),\n",
    "            \"score\": p[\"data\"][\"score\"],\n",
    "            \"num_comments\": p[\"data\"][\"num_comments\"],\n",
    "            \"url\": p[\"data\"][\"url\"]\n",
    "        }\n",
    "        for p in data\n",
    "    ]\n",
    "\n",
    "subreddits = [\"ChatGPT\", \"ArtificialIntelligence\", \"MachineLearning\", \"Futurology\", \"Technology\"]\n",
    "posts = []\n",
    "for s in tqdm(subreddits):\n",
    "    posts.extend(get_posts(s, limit=200))\n",
    "\n",
    "df = pd.DataFrame(posts)\n",
    "df[\"text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"selftext\"].fillna(\"\")\n",
    "df = df[df[\"text\"].str.len() > 40].drop_duplicates(subset=\"text\")\n",
    "df.to_csv(\"data/raw_reddit_ai.csv\", index=False)\n",
    "print(f\"✅ Collected {len(df)} posts from {len(subreddits)} subreddits.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e8edd-2f9e-49d9-9d30-ed63d64b3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cleaning \n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", t)\n",
    "    t = re.sub(r\"[^a-z\\s]\", \"\", t)\n",
    "    return t\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685a493-5d6c-42c5-b3ce-b5232bacaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering \n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_score\"] = df[\"clean_text\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "def classify_impact(x):\n",
    "    if any(w in x for w in [\"destroy\", \"take over\", \"dangerous\", \"replace us\", \"ultron\"]):\n",
    "        return \"Negative\"\n",
    "    elif any(w in x for w in [\"help\", \"assist\", \"innovate\", \"transform\", \"improve\", \"enhance\"]):\n",
    "        return \"Positive\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def classify_understanding(x):\n",
    "    if any(w in x for w in [\"chatgpt\", \"prompt\", \"api\", \"tool\", \"assistant\"]):\n",
    "        return \"Tool\"\n",
    "    elif any(w in x for w in [\"ultron\", \"alive\", \"entity\", \"robot\", \"sentient\", \"conscious\"]):\n",
    "        return \"Entity\"\n",
    "    return \"Mystery\"\n",
    "\n",
    "mental_words = [\"anxiety\",\"fear\",\"depress\",\"excited\",\"addicted\",\"overwhelmed\",\"productive\"]\n",
    "df[\"ai_future_impact\"] = df[\"clean_text\"].apply(classify_impact)\n",
    "df[\"ai_understanding_type\"] = df[\"clean_text\"].apply(classify_understanding)\n",
    "df[\"mental_health_keywords\"] = df[\"clean_text\"].apply(lambda x:any(w in x for w in mental_words))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fff7d5-2496-477b-a4ec-85c561f009d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. TF-IDF\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=3000)\n",
    "X_text = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "print(\"TF-IDF shape:\", X_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77363279-b87b-4bc2-9aae-d6db82a4f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Clustering \n",
    "\n",
    "n_clusters = min(4, len(df))  # avoid >samples\n",
    "dist_matrix = cosine_distances(X_text)\n",
    "model = AgglomerativeClustering(n_clusters=n_clusters, metric=\"precomputed\", linkage=\"average\")\n",
    "df[\"cluster_id\"] = model.fit_predict(dist_matrix)\n",
    "print(\"Clustering done with\", n_clusters, \"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718a0a8-648d-4aa8-8f84-9893acc4c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. SAFE SILHOUETTE / ELBOW ANALYSIS \n",
    "if len(df) > 10:\n",
    "    sil_scores = []\n",
    "    ks = range(2,8)\n",
    "    for k in ks:\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = km.fit_predict(X_text)\n",
    "        if len(set(labels)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            sil = silhouette_score(X_text, labels)\n",
    "            sil_scores.append((k, sil))\n",
    "        except ValueError as e:\n",
    "            print(f\"⚠️ Skipped k={k}: {e}\")\n",
    "    if sil_scores:\n",
    "        plt.plot([k for k,_ in sil_scores],[s for _,s in sil_scores],marker=\"o\")\n",
    "        plt.title(\"Silhouette Scores vs k\")\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Too few samples for silhouette analysis. Add more posts first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84a250-b3c1-4e66-b2f8-627156d9d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Cluster Summary \n",
    "\n",
    "summary = df.groupby(\"cluster_id\").agg({\n",
    "    \"sentiment_score\":\"mean\",\n",
    "    \"ai_future_impact\":lambda x:x.value_counts().index[0],\n",
    "    \"ai_understanding_type\":lambda x:x.value_counts().index[0],\n",
    "    \"mental_health_keywords\":\"sum\"\n",
    "}).reset_index()\n",
    "print(summary)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(data=summary,x=\"cluster_id\",y=\"sentiment_score\",palette=\"coolwarm\")\n",
    "plt.title(\"Average Sentiment per Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ffb11-9223-42a8-aa85-d988b515dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. WORDCLOUDS\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in sorted(df[\"cluster_id\"].unique()):\n",
    "    text_blob = \" \".join(df[df[\"cluster_id\"]==i][\"clean_text\"])\n",
    "    wc = WordCloud(width=600,height=400,background_color=\"white\").generate(text_blob)\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Cluster {i} Word Cloud\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08158991-b25e-46f8-a0a4-18403ca73d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. SAVE \n",
    "df.to_csv(\"outputs/clustered_ai_perceptions.csv\", index=False)\n",
    "summary.to_csv(\"outputs/cluster_summary.csv\", index=False)\n",
    "print(\"Saved results to outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435575e1-0538-4480-8a82-3bce30506717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d15f90-1288-4cd8-bf62-846bfe08bbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
